{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beadf094",
   "metadata": {},
   "source": [
    "## Steps:\n",
    "\n",
    "1. process text\n",
    "2. clean text\n",
    "3. tokenize text and create sequences with Keras\n",
    "4. create LSTM based model\n",
    "5. split text to features and labels: X features => first $n$ words of a sequence, Y label => first word after a sequence\n",
    "6. fit the model\n",
    "7. save the model and toenkizer\n",
    "8. load model and tokenizer\n",
    "9. generate new text based off a seed input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb29a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    with open(filepath) as f:\n",
    "        str_text = f.read()\n",
    "    \n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4efb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_file('moby_dick_four_chapters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d13bd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install spacy -y\n",
    "# !python -m spacy download en\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en_core_web_md\n",
    "# !python -m spacy download en_core_web_lg\n",
    "# !conda install nltk -y\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c545d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bface1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# only need for tokenization and cleaning\n",
    "\n",
    "nlp = spacy.load('en_core_web_md', disable=['parser','tagger','ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19938433",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 1198623 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f8a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punc(doc_text):\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f89e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = read_file('melville-moby_dick.txt')\n",
    "d = read_file('moby_dick_four_chapters.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = separate_punc(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec19b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a2e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: 25 words --> target: next word\n",
    "train_len = 25+1\n",
    "\n",
    "text_sequences = []\n",
    "\n",
    "for i in range(train_len, len(tokens)):\n",
    "    # since training length is n + 1, the\n",
    "    # seq variable is n+1 tokens from index 0 to n \n",
    "    seq = tokens[i-train_len:i]\n",
    "    \n",
    "    text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f9850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "type(text_sequences), len(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328619ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('seq 0:', ' '.join(text_sequences[0]))\n",
    "print('seq 1:', ' '.join(text_sequences[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ed5ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fcb6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11199014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this maps words in strings/sequences to numbers\n",
    "# these numbers represent IDs for a particular word\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c2a947",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in sequences[0]:\n",
    "    print(f\"{idx:5} : {tokenizer.index_word[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13588c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"idx, count: word\")\n",
    "print(f\"----------------\")\n",
    "j=0\n",
    "for idx, (k,v) in enumerate([(k,v) for k,v in tokenizer.word_counts.items()]):\n",
    "    print(f\"{j:3}, {v:5}: {k:15}\")\n",
    "    \n",
    "    j+=1\n",
    "    if j > 10:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9492d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8abab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "type(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ac09c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c3f39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b6bed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b083784",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5f1bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a8045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab everything except the last word from each colum\n",
    "X = sequences[:, :-1]\n",
    "\n",
    "# grab just the last column\n",
    "y = sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3353c3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y,num_classes=vocabulary_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcab9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9136491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6497f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba76510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6e6c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary_size, seq_len, multiplier=4):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocabulary_size, output_dim=seq_len, input_length=seq_len))\n",
    "    model.add(LSTM(seq_len*multiplier, return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(seq_len*multiplier, return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(seq_len*multiplier))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(seq_len*multiplier, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(vocabulary_size, activation = 'softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48b46f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(vocabulary_size+1, seq_len,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa493ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3641c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,y, batch_size = 256, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('test_my_mobydick_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f8ae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(tokenizer,open('test_my_simple_tokenizer','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47fb454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a3ee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    output_text = []\n",
    "    \n",
    "    input_text = seed_text\n",
    "    \n",
    "    for i in range(num_gen_words):\n",
    "        \n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        \n",
    "        pad_encoded = pad_sequences([encoded_text],maxlen=seq_len, truncating='pre')\n",
    "        \n",
    "        predict_probs=model.predict(pad_encoded)\n",
    "        pred_word_index=np.argmax(predict_probs,axis=1)[0]\n",
    "        \n",
    "#         pred_word_index = model.predict_classes(pad_encoded,verbose=0)[0]\n",
    "        \n",
    "        pred_word = tokenizer.index_word[pred_word_index]\n",
    "        \n",
    "        input_text += ' ' + pred_word\n",
    "        \n",
    "        output_text.append(pred_word)\n",
    "    \n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cfe301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(23)\n",
    "random_pick = random.randint(0,len(text_sequences))\n",
    "random_pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a32c6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed_text=text_sequences[random_pick]\n",
    "random_seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f2c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = ' '.join(random_seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2885c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a43e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with 2 epochs\n",
    "generate_text(model, tokenizer, seq_len, seed_text=seed_text,num_gen_words = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e059245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d1c32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5935883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96970ddc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tic = time.perf_counter()\n",
    "model.fit(X,y, batch_size = 128, epochs=300, verbose=1)\n",
    "toc = time.perf_counter()\n",
    "model.save('test_epochBIG.h5')\n",
    "dump(tokenizer,open('test_epochBIG','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b8f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Trained the model in {(toc - tic)/60:0.4f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df390744",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('test_epochBIG.h5')\n",
    "tokenizer = load(open('test_epochBIG','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde32a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with 500 epochs\n",
    "generate_text(model, tokenizer, seq_len, seed_text=seed_text,num_gen_words = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8011ac79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
