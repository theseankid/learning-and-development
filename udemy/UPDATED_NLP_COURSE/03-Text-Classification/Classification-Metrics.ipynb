{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db4e6913",
   "metadata": {},
   "source": [
    "[Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "\n",
    "[Precision and Recall](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
    "\n",
    "[Type I and Type II Errors](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors)\n",
    "\n",
    "[Hypothesis Testing](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing)\n",
    "\n",
    "---\n",
    "\n",
    "Type I Error = false positive (FP)\n",
    "\n",
    "Type II error = false negative (FN)\n",
    "\n",
    "---\n",
    "\n",
    "Q: Overall, how often is a model correct?\n",
    "\n",
    "A: Accuracy, (TP + TN)/ (P + N)\n",
    "\n",
    "---\n",
    "\n",
    "Q: Overall how often is a model wrong?\n",
    "\n",
    "A: Misclassification rate (Error Rate), (FP + FN)/ (P + N) = 1 - Accuracy\n",
    "\n",
    "---\n",
    "\n",
    "Q: Of all the positive predictions, how many were correct?\n",
    "\n",
    "\n",
    "A: Precision, TP / (TP + FP)\n",
    "\n",
    "(probability of a Type I Error)\n",
    "\n",
    "---\n",
    "\n",
    "Q: Of all the positive conditions, how many did we predict was correct?\n",
    "\n",
    "A: Recall, TP / (TP + FN)\n",
    "\n",
    "(probability of a Type II Error)\n",
    "\n",
    "---\n",
    "\n",
    "\"Classification report\" prints out Precision, Recall and F1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f390e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
